---
title: "Tidy Data Preprocessing in R"
subtitle: "Workshop: How to get the most out of Tidymodeling with R"
author: "Henry Baker (228755), Isabella add others."
date: "`r Sys.Date()`"
output: 
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(recipes)
library(ggplot2)
library(lattice)
library(formatR)
library(tidymodels)
library(dplyr)
```    


## 

<center>


# *"To Tidymodel or not to Tidymodel, that is the question..."* <!-- .element: class="fragment" data-fragment-index="1" -->

# - Hadley Wickam (probably) <!-- .element: class="fragment" data-fragment-index="2" -->

---

### Presented by Henry Baker, Isabella, add others
### ADD GROUP <!-- .element: class="fragment" data-fragment-index="3" -->
### October 2, 2023

<div style="text-align: center;">
  <img src="pics/tidymodels_packages.jpg" style="max-height: 250px; opacity: 0.5;">
</div>

## Agenda

1. **Why Tidymodeling? (Goodbye Base R)**
2. **What is Tidymodeling?**
3. **Key concepts in Tidymodeling**
4. **Out of Scope**
5. **The Power of 'recipes'**
6. **Interactive Coding Session**

# So, why Tidymodeling?
## Remember this?

```{r, echo=FALSE}
# non-visible 
set.seed(123) 

synthetic_data <- data.frame(
  predictor1 = rnorm(100),  
  predictor2 = rnorm(100),  
  predictor3 = rnorm(100),  
  control1 = sample(c("Yes", "No"), 100, replace = TRUE), 
  control2 = runif(100), 
  outcome = rnorm(100)  
)

synthetic_data$outcome <- with(synthetic_data, predictor1 + 2*predictor2 + rnorm(100))


```

```{r, echo=TRUE}
model <- lm(outcome ~ predictor1 + predictor2 + predictor3 + control1 + control2, data=synthetic_data)
summary(model)

```

## The Challenge with Base R's 'Formula'

- **Inconsistent syntax across functions** - can’t swap in / out, to compare across models easily,
- Preprocessing a **separate data wrangling excercise**, 
- Preprocessing often involves a **mix of functions** and approaches, 
- **Lack of a standardized workflow**,
- Lack of standardized framework for evaluation & tuning hyperparameters
- Outputs are ugly,
- Outputs not always a dataframe,
- Difficulties in maintaining and scaling analyses

![](path_to_a_relevant_image.png) <!--Replace this with the path to your image-->

And after all that, having **finally** run your model(s)...

# ...what if you realized you’d been heading in the wrong direction all this time and wanted to run a *different* model...
## ...perhaps all along you wanted to run a:
- Decision Trees (`rpart` or `randomForest`)
- Poisson Regression
- Neural net (`keras`)
- Time series

## Worse still, R’s strength is also its greatest weakness…

One of the strengths of R is that it encourages developers to create a user interface that fits their needs. 

As an example, here are three common methods for creating a scatter plot of two numeric variables in a data frame called `plot_data`:

<center>

# 1) BASE R 
```{r eval = F}
plot(data$x, data$y, main="Base R Plot", pch=19, col=rgb(0.2,0.4,0.6,0.6))
```

</center>

<center>

# 2) LATTICE 
```{r eval = F}
xyplot(y ~ x, data = data, 
                       main = "Scatter Plot with Lattice",
                       xlab = "Independent", 
                       ylab = "Dependent",
                       col = "blue",
                       pch = 16) 
```

</center>

<center>

# 3) GGPLOT 
```{r eval = F}
ggplot(data, aes(x = x, y = y)) + 
  geom_point(color = 'blue') +
  geom_smooth(method = 'lm', se = FALSE, color = 'red') + 
  labs(
    title = "Generated Scatter Plot with Smooth Line",
    x = "Independent",
    y = "Dependent"
  ) +
  theme_minimal()  

```

</center>


---

In these three cases, separate groups of developers devised three distinct interfaces for the same task. Each has advantages and disadvantages.


In comparison, the Python Developer’s Guide espouses the notion that, when approaching a problem:
*“There should be one – and preferably only one – obvious way to do it.”*

# Introducing Tidymodeling: A Paradigm Shift

## What is(/are) ‘Tidymodels’

Tidymodels is a collection of R packages and a framework for modeling and machine learning that follows the principles of tidy data and integrates seamlessly with the tidyverse ecosystem. 

It was developed to provide a consistent and organized way to perform machine learning tasks in R, making it easier for data scientists and analysts to build, evaluate, and deploy predictive models.”

## Tidy Design

- **Unifies** various modeling functions under a single roof
- **Simplifies** the data **preprocessing pipeline**
- Introduces a consistent syntax and workflow
- Enhances readability and maintenance of code
- Empowers both novice and experienced data scientists

![](path_to_tidymodeling_benefits_image.png) <!--Replace with the path to your image-->

Transitioning from the world of scattered functions and inconsistent methodologies, tidymodeling simplifies your workflow but also introduces an elegance to data preprocessing. 

With tidymodeling, you no longer need to be a 'jack of all trades' — the package suite skillfully integrates essential tools, providing a cohesive experience. 


## Just as Tidyverse syntax > base R expressions….

- **Tidyverse integration**: Built on the principles of the tidyverse, promoting consistent and user-friendly data manipulation. Familiarity with the tidyverse makes using Tidymodels seamless and consistent in data analysis and modeling workflows.

- **Consistency & workflow**: Tidymodels ensures a structured workflow for modeling, encapsulating data pre-processing, model specification, tuning, and evaluation, enhancing organization and transparency.

- **Recipes for data pre-processing**: The inclusion of the `recipes` package in Tidymodels allows structured, reproducible data pre-processing steps, beneficial for feature engineering and data transformation.

- **Model Agnosticism**: Facilitates easy swapping of different models for experimentation and selection without extensive code adjustments.

- **Hyperparameter Tuning**: Streamlined process for adjusting and finding the best model parameters.

- **Resampling & Cross-validation**: Offers resampling methods, like cross-validation and bootstrapping, essential for reliable estimations of model generalization.

- **Extensive metrics**: The `yardstick` package in Tidymodels provides a broad spectrum of evaluation metrics, simplifying model performance comparison and assessment.

## Key Concepts in Tidymodeling (I) - Preprocessing with a 'Recipe'

- A 'recipe' prepares your data for fitting a model.
  
### 1) Data Splitting with `rsample`

- Crucial for assessing and tuning your models.
- `rsample` splits your dataset into training, validation, and test sets.
- Note: Evaluation and tuning models are out of scope for this workshop but are vital parts of the workflow.

```{r eval = F}
data_split <- initial_split(flight_data, prop = 3/4) 
train_data <- training(data_split)
test_data <- testing(data_split)
```

### 2) Assigning "Roles" with `update_role()`: 

- Optional step to assign roles, e.g., "ID" to variables.
- ID variables are not included in the model but are retained for investigation purposes.

---

### 3) Feature Engineering with 'step_XYZ()': 

Feature engineering involves several preprocessing steps essential for making your data suitable for modeling.

### Common Preprocessing Tasks:

- **Data Normalization:** Standardizes the range of independent variables.
- **One-hot encoding:** Converts categorical variables into a form that could be provided to ML algorithms.
- **Handling missing values:** Techniques to handle missing data (e.g., imputation).

### Examples:

1. **Converting qualitative predictors to indicator variables:** Often called dummy coding, this process changes categorical variables into a series of binary columns.

2. **Transforming data scales:** For example, taking the logarithm of a variable to reduce skewness.

3. **Group transformations:** Applying a transformation to a group of variables together, preserving their relational information.

4. **Feature extraction from raw variables:** For instance, extracting the day of the week from a date variable, which can be particularly useful for time series analysis or capturing seasonal trends.

*Note:* These techniques help in refining the features present in the data, enhancing the predictive quality of models.


## Key Concepts in Tidymodeling (II) - Iterative Modeling

### 1) Model Specification:
- with `parsnip`: type of model and hyperpermaters
```{r, eval = FALSE}
lr_mod <- 
	logistic_reg() %>%
	 set_engine("glm")
```

### 2) Workflow:
- use a *`model workflow`*, which pairs a model and a recipe together

```{r, eval = FALSE}
flights_wflow <- 
	workflow() %>% 
	add_model(lr_mod) %>% 
	add_recipe(flights_rec) 
```

### 3) Fit & Train model:
-  the `fit` function that can be used to prepare the recipe and train the model from the resulting predictors:

```{r, eval = FALSE}
flights_fit <- 
  flights_wflow %>% 
  fit(data = train_data)
```

## Key Concepts in Tidymodeling (III) - Post-Processing 

- selecting the best model configuration, and fine-tuning hyperparameters. Some useful steps include:

### 1) Evaluating / estimating performance, 
- with `broom`: use broom to tidy up model results, making them easier to interpret and visualize.
- with `yardstick`: Use yardstick to calculate evaluation metrics like RMSE, MAE, or ROC AUC.
- resampling with `??`

### 2) Hyperparameter Tuning 
- with `tune` and `dials`: Fine-tune model hyperparameters to optimize performance based on evaluation metrics.



## Out of Scope (but nonetheless important)

- complex modeling
- post processing - a major benefit from Tidymodeling

## Focus: Harnessing the Power of 'recipes'

- What are 'recipes'?
  + A powerful specification for data preprocessing
  + Enables creation of feature engineering steps
  + Maintains a blueprint of the data transformation process

- Why 'recipes'?
  + Streamlines complex preprocessing tasks
  + Ensures consistency in data preparation
  + Facilitates easy reproduction of models

- Integration in Tidymodeling
  + Seamless interaction with other tidymodel packages
  + Becomes a central component of the analysis pipeline

![](path_to_recipes_visualization.png) <!--Replace with the path to your image-->

# recipes

### 'Recipes' in Action: A Walkthrough

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Sample data
data(mtcars)

# Starting the recipe
rec <- recipe(mpg ~ ., data = mtcars) 

# Data preprocessing steps
rec <- rec %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_numeric()) %>%
  step_dummy(all_nominal())

# Preprocessed data summary
summary(prepped <- prep(rec, training = mtcars))
```
---
Demonstrates normalization, PCA, and dummy coding
step_* functions for different transformations
Easy reproduction and consistency in data analysis

Note: Witness 'recipes' in its true element through this walkthrough. Here, we're taking the renowned 'mtcars' dataset and applying a series of transformations, all within the tidy, understandable structure that 'recipes' provides. Note how each 'step_*' function corresponds to a specific transformation, making your preprocessing steps modular, clear, and easy to adjust or reproduce. This hands-on example showcases the practical elegance of 'recipes' in data preprocessing.

## From Preprocessing to Modeling: A Seamless Journey

- Tidymodeling: Not just preprocessing
  + A unified framework for all stages of data analysis
  + Modeling, validation, and interpretation within the same ecosystem

- Transitioning with ease
  + Direct passage from 'recipes' preprocessing to model input
  + No need for manual data reshaping or reformatting

- Compatibility and Flexibility
  + Works with a range of model types (e.g., regression, classification)
  + Accommodates various validation techniques

![](path_to_workflow_visualization.png) <!--Replace with the path to your image-->


Note: The true power of tidymodeling becomes evident in its holistic approach. It's not just about individual steps; it's the journey from raw data to insightful models, all within the same, consistent environment. One of tidymodeling's most compelling advantages is the ease of transition between stages of data analysis. It eliminates the disjointed processes often encountered in traditional methods, replacing them with a streamlined, intuitive workflow. And with its broad compatibility, tidymodeling ensures that you have the tools you need, regardless of your specific modeling or validation requirements.

## To infinity & Beyond: onward reading...

- [Tidymodels documentation](https://www.tidymodels.org/), a quick overview of the main packages, features and workflow
- [Tidy Modeling with R Text Book](https://www.tmwr.org/), Comprehensive text book featuring extensive example using Ames Housing Data set and ...... by Max Kuhn & Julia SIlge, 2023
]

## Getting More Hands on 

Install the complete tidymodeling package set with:

```{r, eval = FALSE}
install.packages("tidymodel")
```

Library()

```{r, eval = FALSE}
library(tidymodels)
```

### Exploring the packages:

```{r tverse_pkgs}
tidymodels_packages()
```
We see that we have actually loaded a number of packages (which could also be loaded individually), some core ones: `recipe`, `parsnip`, `workflows`, `yardstick` etc.

## Core functions within Tidymodels

**dials():** has tools to create and manage values of tuning parameters.

**infer():** is a modern approach to statistical inference.

**parsnip()**: is a tidy, unified interface to creating models.

**recipe()**: is a general data preprocessor with a modern interface. It can create model matrices that incorporate feature engineering, imputation, and other help tools.

**rsample()**: has infrastructure for resampling data so that models can be assessed and empirically validated.
**tune**: contains the functions to optimize model hyper-parameters.

**workflows()**: has methods to combine pre-processing steps and models into a single object.

**yardstick()**: contains tools for evaluating models (e.g.accuracy, RMSE, etc.).

- In addition to the currently core tidymodels packages.... usage.<sup>1</sup>
- See [here](https://www.tidymodels.org/find/) for a list of all tidymodels functions across different CRAN packages can be found at, or just in R directly:


